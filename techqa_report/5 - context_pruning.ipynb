{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a874a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "retrieved_contexts_file_path = \"/kaggle/input/retrieved-contexts/retrieved_contexts_45.json\"\n",
    "benchmark_file_path = \"/kaggle/input/benchmark-techqa/benchmark_query_rewriting.json\"\n",
    "output_file_path = \"retrieved_contexts_45_pruned.json\"\n",
    "\n",
    "# Open input files\n",
    "with open(retrieved_contexts_file_path, \"r\") as file:\n",
    "    retrieved_contexts = json.load(file)\n",
    "    \n",
    "with open(benchmark_file_path, \"r\") as benchmark_file:\n",
    "    benchmark_instances = json.load(benchmark_file)\n",
    "\n",
    "# Set GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the pruning model\n",
    "pruning_model = AutoModel.from_pretrained(\"naver/provence-reranker-debertav3-v1\", trust_remote_code=True).to(device)\n",
    "\n",
    "new_retrieved_contexts = []\n",
    "quest_count = 0\n",
    "for benchmark_instance, retrieved_documents  in zip(benchmark_instances, retrieved_contexts):\n",
    "    \n",
    "    if quest_count % 50 == 0:\n",
    "        print(str(quest_count))\n",
    "    quest_count += 1\n",
    "    \n",
    "    new_question = benchmark_instance[\"rewrited_question\"]\n",
    "\n",
    "    new_context = []\n",
    "    for document in retrieved_documents:\n",
    "        # Retrieve all sections text of the document\n",
    "        section_texts = []\n",
    "        for section in document[\"sections\"]:\n",
    "            if section[\"section_text\"] != \"\":\n",
    "                section_texts.append(section[\"section_text\"])\n",
    "\n",
    "        # Prune sections text\n",
    "        new_sections = []\n",
    "        pruning_model_outputs = pruning_model.process(new_question, [section_texts])[\"pruned_context\"][0]\n",
    "        for pruned_text, section in zip(pruning_model_outputs, document[\"sections\"]):\n",
    "            new_sections.append(section)\n",
    "            # Add pruned text to section\n",
    "            section[\"pruned_context\"] = pruned_text\n",
    "            \n",
    "        # Create and populate the new document\n",
    "        new_document = document\n",
    "        new_document[\"sections\"] = new_sections\n",
    "        new_context.append(new_document)\n",
    "        \n",
    "    new_retrieved_contexts.append(new_context)\n",
    "\n",
    "# Save new contexts in the output file\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    json.dump(new_retrieved_contexts, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
