{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "retrieved_contexts_file_path = \"/kaggle/input/retrieved-contexts-45-no-pruning/retrieved_contexts_45.json\"\n",
    "benchmark_file_path = \"/kaggle/input/benchmark-techqa/benchmark_query_rewriting.json\"\n",
    "generated_answers_file_path = \"/kaggle/working/generated_answers.json\"\n",
    "\n",
    "with open(retrieved_contexts_file_path, \"r\") as file:\n",
    "    retrieved_contexts = json.load(file)\n",
    "\n",
    "with open(benchmark_file_path, \"r\") as benchmark_file:\n",
    "    benchmark_instances = json.load(benchmark_file)\n",
    "\n",
    "# Prompt message for LLM\n",
    "prompt = \"\"\"\n",
    "You are Granite, an AI developed by IBM. You are a helpful RAG (Retrieval-Augmented Generation) system designed to answer user queries based only on the content of the retrieved documents.\n",
    "\n",
    "### Key Instructions:\n",
    "1. **Only Use Retrieved Documents for Answers**: Provide an answer using specific, direct information in the retrieved documents. \n",
    "2. **No Speculation**: Do not try to make inferences or use general knowledge.\n",
    "3. **Do Not Mention Documents**: Never refer to, mention, or include any details about the documents in your response. Do not say things like \"According to the document,\" or \"The document indicates...\". Simply provide the answer.\n",
    "4. **No Extra Information**: Do not elaborate or provide additional context.\n",
    "\"\"\"  \n",
    "\n",
    "# Setup the LLM to generate answers\n",
    "model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "max_len = 16384\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = LLM(model=model_name, tensor_parallel_size=2, trust_remote_code=True, max_model_len = max_len, gpu_memory_utilization=0.90)\n",
    "sampling_params = SamplingParams(temperature=0.05, max_tokens=8192, stop_token_ids=[tokenizer.eos_token_id])\n",
    "\n",
    "\n",
    "quest_count = 0\n",
    "generated_answers = []\n",
    "all_messages = []\n",
    "all_document_texts = [] \n",
    "answerable_queries = []\n",
    "correct_answers = []\n",
    "for benchmark_instance, retrieved_documents in zip(benchmark_instances, retrieved_contexts):\n",
    "    # Get only answerable questions\n",
    "    if benchmark_instance[\"is_impossible\"] == True:\n",
    "        continue\n",
    "    \n",
    "    if quest_count % 25 == 0:\n",
    "        print(quest_count)\n",
    "\n",
    "    answerable_queries.append(benchmark_instance[\"question\"])\n",
    "    correct_answers.append(benchmark_instance[\"answer\"])\n",
    "    \n",
    "    quest_count += 1\n",
    "    document_texts = []\n",
    "    text_len = 0\n",
    "    # Get text of all documents\n",
    "    for document in retrieved_documents:\n",
    "        document_text = \"\"\n",
    "        for section in document[\"sections\"]:\n",
    "            document_text = document_text + section[\"section_text\"]\n",
    "        text_len = text_len + len(document_text)\n",
    "        document_texts.append(document_text)\n",
    "    \n",
    "    # Create chat_template for the LLM\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\" : prompt},\n",
    "        {\"role\": \"user\", \"content\": benchmark_instance[\"question\"]},\n",
    "        {\"role\": \"system\", \"content\" : \"Retrieved Documents:\" + str(document_texts)}        \n",
    "    ]\n",
    "        \n",
    " \n",
    "    all_messages.append(messages)\n",
    "    all_document_texts.append(document_texts)\n",
    "\n",
    "\n",
    "print(\"Created all messages\")\n",
    "\n",
    "# Generate all the answers for answerable questions\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(message, add_generation_prompt=True)[:max_len] for message in all_messages]\n",
    "\n",
    "outputs = None\n",
    "with torch.no_grad():\n",
    "    outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "# Get the text of all the generated answers\n",
    "llm_answers = [output.outputs[0].text for output in outputs]\n",
    "for llm_answer, query, context, correct_answer in zip(llm_answers, answerable_queries, all_document_texts, correct_answers):\n",
    "    # Prepare the output\n",
    "    generated_answers.append(\n",
    "        {\n",
    "            \"user_input\": query, \n",
    "            \"retrieved_contexts\": context, \n",
    "            \"answer\": llm_answer, \n",
    "            \"reference\" : correct_answer\n",
    "        }\n",
    "    )\n",
    "\n",
    "with open(generated_answers_file_path, \"w\") as file:\n",
    "    json.dump(generated_answers, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
