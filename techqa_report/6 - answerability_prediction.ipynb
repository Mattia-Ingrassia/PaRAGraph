{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "retrieved_contexts_file_path = \"/kaggle/input/retrieved-contexts/retrieved_contexts_45.json\"\n",
    "benchmark_file_path = \"/kaggle/input/benchmark-techqa/benchmark_query_rewriting.json\"\n",
    "file_path_out = \"/kaggle/working/answerable_questions.json\"\n",
    "\n",
    "# Open input files\n",
    "with open(retrieved_contexts_file_path, \"r\") as file:\n",
    "    retrieved_contexts = json.load(file)\n",
    "    \n",
    "with open(benchmark_file_path, \"r\") as benchmark_file:\n",
    "    benchmark_instances = json.load(benchmark_file)\n",
    "\n",
    "# Answerability tag for LoRA\n",
    "ANSWERABILITY_PROMPT = \"<|start_of_role|>answerability<|end_of_role|>\"\n",
    "\n",
    "# Load models\n",
    "model_name = \"ibm-granite/granite-3.2-8b-instruct\"\n",
    "LORA_NAME = \"ibm-granite/granite-3.2-8b-lora-rag-answerability-prediction\"\n",
    "    \n",
    "# Set up GPUs if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Usando {torch.cuda.device_count()} GPU\")\n",
    "    device_map = \"auto\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "# Quantization in 8bit for reducing model size\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True, \n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type=\"nf8\"\n",
    ")\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\", trust_remote_code=True)\n",
    "    \n",
    "# Load base model\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    quantization_config=quantization_config, \n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Load answerability model\n",
    "model_answerability = PeftModel.from_pretrained(model_base, LORA_NAME)\n",
    "\n",
    "answerable_questions = []\n",
    "cont_ques = 0\n",
    "for benchmark_instance, retrieved_documents in zip(benchmark_instances, retrieved_contexts):\n",
    "\n",
    "    if cont_ques % 50 == 0:\n",
    "        print(str(cont_ques))\n",
    "    cont_ques = cont_ques + 1\n",
    "    \n",
    "    # Rebuild document's text\n",
    "    documents = []\n",
    "    for document in retrieved_documents:\n",
    "        document_text = \"\"\n",
    "        for section in document[\"sections\"]:\n",
    "            document_text += section[\"section_text\"]\n",
    "        documents.append({\"text\": document_text, \"document_id\": document[\"document_id\"]})\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": benchmark_instance[\"question\"]}\n",
    "    ]\n",
    "\n",
    "    # Prepare answerability prompt\n",
    "    string = tokenizer.apply_chat_template(messages, documents=documents, tokenize=False, add_generation_prompt=False)\n",
    "    inputs = string + ANSWERABILITY_PROMPT\n",
    "    \n",
    "    inputs_tokenized = tokenizer(\n",
    "        inputs, \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True, \n",
    "        max_length=32768\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs_tokenized[\"input_ids\"].to(device)\n",
    "    # Avoid gradient calculation during inference since its not needed\n",
    "    with torch.no_grad():\n",
    "        # Generate answerability prediction using LoRA\n",
    "        output = model_answerability.generate(\n",
    "            input_ids,\n",
    "            attention_mask=inputs_tokenized[\"attention_mask\"].to(device),\n",
    "            max_new_tokens=3,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    generated_only_ids = output[0][input_ids.shape[-1]:]\n",
    "    answerability = tokenizer.decode(generated_only_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    if \"unanswerable\" in answerability:\n",
    "        answerable_questions.append(0)\n",
    "    else:\n",
    "        answerable_questions.append(1)\n",
    "    \n",
    "print(\"Ended answerability evaluation\")\n",
    "\n",
    "with open(file_path_out, \"w\") as file:\n",
    "    json.dump(answerable_questions, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
