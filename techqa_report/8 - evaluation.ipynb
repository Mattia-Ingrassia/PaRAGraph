{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric, ContextualRecallMetric, ContextualRelevancyMetric\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import regex\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answers_path = \"/kaggle/input/generated-answers/generated_answers.json\"\n",
    "benchmark_file_path = \"/kaggle/input/benchmark-techqa/benchmark_query_rewriting.json\"\n",
    "answerable_file_path = \"/kaggle/input/answerability/answerable_questions.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d0be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(benchmark_file_path, \"r\") as file:\n",
    "    benchmark_instances = json.load(file)\n",
    "\n",
    "with open(answerable_file_path, \"r\") as file:\n",
    "    answerable_questions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "TP = TN = FP = FN = 0\n",
    "\n",
    "for is_answerable, benchmark_instance in zip(answerable_questions, benchmark_instances):\n",
    "    is_impossible = benchmark_instances[\"is_impossible\"]\n",
    "    \n",
    "    predicted_impossible = False if is_answerable == 1 else True\n",
    "   \n",
    "    # Count TP, TN, FP, FN based on conditions\n",
    "    if is_impossible and predicted_impossible:\n",
    "        TP += 1\n",
    "    elif is_impossible and not predicted_impossible:\n",
    "        FN += 1\n",
    "    elif not is_impossible and predicted_impossible:\n",
    "        FP += 1\n",
    "    elif not is_impossible and not predicted_impossible:\n",
    "        TN += 1\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "# Compute metrics for positive class (unanswerable)\n",
    "\n",
    "precision_pos = TP / (TP + FP)\n",
    "recall_pos = TP / (TP + FN)\n",
    "f1_score_pos = 2 * (precision_pos * recall_pos) / (precision_pos + recall_pos)\n",
    "\n",
    "# Compute metrics for negative class (answerable)\n",
    "precision_neg = TN / (TN + FN)\n",
    "recall_neg = TN / (TN + FP)\n",
    "f1_score_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg)\n",
    "\n",
    "# Print the results\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"Precision unanswerable: {precision_pos:.4f}\")\n",
    "print(f\"Recall unanswerable: {recall_pos:.4f}\")\n",
    "print(f\"F1-Score unanswerable: {f1_score_pos:.4f}\")\n",
    "\n",
    "print(f\"Precision answerable: {precision_neg:.4f}\")\n",
    "print(f\"Recall answerable: {recall_neg:.4f}\")\n",
    "print(f\"F1-Score answerable: {f1_score_neg:.4f}\")\n",
    "\n",
    "macro_f1 = (f1_score_pos + f1_score_neg) / 2\n",
    "print(f\"macro_f1: {macro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluator class\n",
    "\n",
    "class CustomEvaluator(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_name, max_len = 16834):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = LLM(model=model_name, tensor_parallel_size=2, trust_remote_code=True, max_model_len = max_len, gpu_memory_utilization=0.90)\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel = None) -> BaseModel | str:\n",
    "        llm = self.load_model()\n",
    "\n",
    "        json_prompt = \"\"\n",
    "        if schema is None:\n",
    "            # The generated answer will be a normal string\n",
    "            sampling_params = SamplingParams(temperature=0.2, max_tokens=8096, stop_token_ids=[self.tokenizer.eos_token_id])\n",
    "        else:\n",
    "            # The generated answer will be a JSON document\n",
    "            json_schema = schema.model_json_schema()\n",
    "            guided_decoding_params = GuidedDecodingParams(json = json_schema)\n",
    "            sampling_params = SamplingParams(temperature=0.15, max_tokens=8096, guided_decoding = guided_decoding_params)\n",
    "            json_prompt = \"You must always respond only with valid JSON that matches the provided schema. No explanations or extra text.\" \n",
    "        \n",
    "        # Create messages for the model\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt + json_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Generate model answer \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            generated_text = llm.generate(prompt_token_ids=text, sampling_params=sampling_params)[0].outputs[0].text\n",
    "        \n",
    "        if schema is None:\n",
    "            #String output\n",
    "            return generated_text \n",
    "        \n",
    "        # JSON output \n",
    "        match = regex.search(r\"\\{(?:[^{}]|(?R))*\\}\", generated_text, regex.DOTALL)  # match JSON from first { to its closing }\n",
    "        if match is not None:\n",
    "            json_result = json.loads(match.group(0))\n",
    "            return schema(**json_result)\n",
    "        return None\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel = None) -> BaseModel | str:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"CustomEvaluator-\" + self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom evaluator\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "custom_evaluator = CustomEvaluator(model_name)\n",
    "\n",
    "# Define relevant metrics\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(\n",
    "    threshold=0.0,\n",
    "    model = custom_evaluator,\n",
    "    include_reason=True,\n",
    "    verbose_mode = False,\n",
    "    async_mode = False\n",
    ")\n",
    "\n",
    "contextual_precision_metric = ContextualPrecisionMetric(\n",
    "    threshold=0.0,\n",
    "    model=custom_evaluator,\n",
    "    include_reason=True,\n",
    "    verbose_mode = False,\n",
    "    async_mode = False\n",
    ")\n",
    "\n",
    "contextual_recall_metric = ContextualRecallMetric(\n",
    "    threshold=0.0,\n",
    "    model=custom_evaluator,\n",
    "    include_reason=True,\n",
    "    verbose_mode = False,\n",
    "    async_mode = False\n",
    ")\n",
    "\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(\n",
    "    threshold=0.0,\n",
    "    model=custom_evaluator,\n",
    "    include_reason=True,\n",
    "    verbose_mode = False,\n",
    "    async_mode = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(generated_answers_path, \"r\") as file:\n",
    "    generated_answers = json.load(file)\n",
    "\n",
    "# Indexes can be edited for specific testing on some questions\n",
    "start_index = 0\n",
    "end_index = len(generated_answers) \n",
    "all_test_cases = []\n",
    "for answer_instance in generated_answers[start_index:end_index]:\n",
    "    question = answer_instance[\"user_input\"]\n",
    "    ground_truth = answer_instance[\"reference\"]\n",
    "    llm_answer = answer_instance[\"answer\"]\n",
    "    retrieved_contexts = answer_instance[\"retrieved_contexts\"]\n",
    "\n",
    "    # Create test case for current answer\n",
    "    test_case = LLMTestCase(\n",
    "        input = question,\n",
    "        actual_output = llm_answer,\n",
    "        retrieval_context = retrieved_contexts,\n",
    "        expected_output = ground_truth\n",
    "    )\n",
    "    all_test_cases.append(test_case)\n",
    "\n",
    "# Evaluate all created tests\n",
    "try:\n",
    "    evaluations = evaluate(test_cases = all_test_cases, metrics = [\n",
    "                                                                    answer_relevancy_metric, \n",
    "                                                                    contextual_precision_metric,\n",
    "                                                                    contextual_recall_metric, \n",
    "                                                                    contextual_relevancy_metric\n",
    "                                                                ])\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating test case: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
