{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-s9lecIPLOz",
        "outputId": "67e72236-0fb6-4638-abb6-99e20bab332f"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OWfSY-SPV_i"
      },
      "outputs": [],
      "source": [
        "original_file_path = \"drive/MyDrive/paragraph/corpus/full_technote_collection.txt\"\n",
        "fixed_file_path = \"drive/MyDrive/paragraph/corpus/full_technote_collection_fixed.txt\"\n",
        "partial_out_path = \"drive/MyDrive/paragraph/corpus/custom_sections.json\"\n",
        "final_out_path = \"drive/MyDrive/paragraph/corpus/custom_sections_512.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZIC5uS4cfHe"
      },
      "outputs": [],
      "source": [
        "TO_ESCAPE = [\"(\", \")\", \"+\", \"*\", \"?\", \"[\", \"]\"]\n",
        "\n",
        "# Functions for escaping de-escaping special chars\n",
        "\n",
        "def escape(s):\n",
        "  s = s.replace(\"\\\\ \",\"\\\\\")\n",
        "  s = s.replace(\" \\\\\",\"\\\\\")\n",
        "  s = s.replace(\"\\\\\",\"\\\\\\\\\")\n",
        "  for escape_char in TO_ESCAPE:\n",
        "    s = s.replace(escape_char, \"\\\\\" + escape_char)\n",
        "  return s\n",
        "\n",
        "def remove_escape(s):\n",
        "  for escape_char in TO_ESCAPE:\n",
        "    s = s.replace(\"\\\\\" + escape_char, escape_char)\n",
        "  s = s.replace(\"\\\\\\\\\",\"\\\\\")\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPG6RPIKSbDR"
      },
      "outputs": [],
      "source": [
        "# Resolving issues caused by wrong structure of some html pages\n",
        "to_skip = [\"swg21634896\"]\n",
        "\n",
        "with open(original_file_path, \"r\") as file:\n",
        "  with open(fixed_file_path, \"a\") as file_out:\n",
        "      for line in file:\n",
        "          line_data = []\n",
        "          line_documents = json.loads(line)\n",
        "          for document in line_documents:\n",
        "              if document[\"id\"] in to_skip:\n",
        "                  continue\n",
        "\n",
        "              if document[\"id\"] == \"swg21652675\":\n",
        "                  document[\"text\"] = document[\"text\"].replace(\"* Windows\", \"* WINDOWS\")\n",
        "                  document[\"text\"] = document[\"text\"].replace(\"* Linux and UNIX\", \"* LINUX AND UNIX\")\n",
        "\n",
        "              if document[\"id\"] == \"isg3T1000214\":\n",
        "                  original_text = '<h2 class=\" ibm-h4 ibm-bold\"><a name=\"1.8\">Deliver the testcase via FTP</a>'\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, original_text + \"</h2>\")\n",
        "\n",
        "              if document[\"id\"] == \"swg27011656\":\n",
        "                  original_text = '<h2 class=\"ibm-rule ibm-h4 ibm-bold\">DB2 9 for z/OS Product Documentation <br> <br>'\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, original_text + \"</h2>\")\n",
        "\n",
        "              if document[\"id\"] == \"swg21672099\":\n",
        "                  document[\"content\"] = document[\"content\"].replace(\"&lt;/H2&gt;]\", \"</h2>\")\n",
        "\n",
        "              if document[\"id\"] == \"isg3T1000260\":\n",
        "                  original_text = '<h2 class=\" ibm-h4 ibm-bold\"> <a name=\"4\"></a> The shell script'\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, original_text + \"</h2>\")\n",
        "\n",
        "              if document[\"id\"] == \"isg3T1000192\":\n",
        "                  original_text = '<h2 class=\" ibm-h4 ibm-bold\"><a name=\"related\">Related documentation</a>'\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, original_text + \"</h2>\")\n",
        "\n",
        "              if document[\"id\"] == \"swg21298716\":\n",
        "                  original_text = '<h2 class=\" ibm-h4 ibm-bold\">If you did not find your question about DB2 listed here then please use the &apos;Rate this page&apos; section below to send the team a question to add to the DB2 Fequently Asked Question list.'\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, original_text + \"</h2>\")\n",
        "\n",
        "              if document[\"id\"] == \"swg21651211\":\n",
        "                  original_text = \"<tt>&#xA0;\"\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, \"\")\n",
        "                  original_text = \"</tt></h2>\"\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, \"</h2>\")\n",
        "\n",
        "              if document[\"id\"] == \"swg22016151\":\n",
        "                  original_text = '<h2 class=\" ibm-h4 ibm-bold\"><b>Data&#xA0;</b><b>r</b><b>equest&#xA0;</b><b>overview</b></h2>'\n",
        "                  new_text = '<h2 class=\" ibm-h4 ibm-bold\"><b>Datarequestoverview</b></h2>'\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, new_text)\n",
        "                  original_text = '<h2 class=\" ibm-h4 ibm-bold\"><b>Set</b><b>ting</b><b>&#xA0;up the&#xA0;</b><b>s</b><b>ystem</b></h2>'\n",
        "                  new_text = '<h2 class=\" ibm-h4 ibm-bold\"><b>Settingup thesystem</b></h2>'\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, new_text)\n",
        "                  original_text = '<h2 class=\" ibm-h4 ibm-bold\"><b>Test&#xA0;</b><b>your d</b><b>ata&#xA0;</b><b>r</b><b>equest</b></h2>'\n",
        "                  new_text = '<h2 class=\" ibm-h4 ibm-bold\"><b>Testyour datarequest</b></h2>'\n",
        "                  document[\"content\"] = document[\"content\"].replace(original_text, new_text)\n",
        "\n",
        "\n",
        "              line_data.append(document)\n",
        "\n",
        "          file_out.write(json.dumps(line_data) + \"\\n\")\n",
        "\n",
        "      file_out.close()\n",
        "\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1jqqlZl67_7D",
        "outputId": "11d11d7a-bf16-4ff7-dace-06cd3b7c412c"
      },
      "outputs": [],
      "source": [
        "# Function for creating a section\n",
        "def create_section(key, text, start_index, end_index):\n",
        "    section = {}\n",
        "    deescaped_key = remove_escape(key)\n",
        "    section[\"title\"] = deescaped_key\n",
        "    section[\"text\"] = text[start_index: end_index]\n",
        "    section[\"start\"] = start_index\n",
        "    section[\"end\"] = end_index\n",
        "    return section\n",
        "\n",
        "# Pattern for removing special chars with regex\n",
        "pattern = r\"\\xa0\"\n",
        "\n",
        "with open(partial_out_path, \"w\") as file_out:\n",
        "  with open(fixed_file_path, \"r\") as file:\n",
        "      for line in file:\n",
        "          line_data = []\n",
        "          line_documents = json.loads(line)\n",
        "          for document in line_documents:\n",
        "              print(\"id:\", document[\"id\"])\n",
        "\n",
        "              soup = BeautifulSoup(document[\"content\"], \"html.parser\")\n",
        "              # Replace all <br> with blank spaces\n",
        "              for br in soup.find_all(\"br\"):\n",
        "                br.replace_with(\" \")\n",
        "\n",
        "              document[\"content\"] = str(soup)\n",
        "\n",
        "              # Convert <a href=\"link.com\"> link </a> --> link [link.com]\n",
        "              for a_href in soup.find_all(\"a\"):\n",
        "                if a_href.get(\"href\") == None or a_href.get(\"href\") == \"#\" or a_href.get(\"href\").find(\"/\") == -1:\n",
        "                  continue\n",
        "                document[\"content\"] = document[\"content\"].replace(str(a_href), a_href.text + \" [\" + a_href.get(\"href\") + \"]\")\n",
        "\n",
        "              keys = []\n",
        "              soup = BeautifulSoup(document[\"content\"], \"html.parser\")\n",
        "              h2s = soup.find_all(\"h2\")\n",
        "              \n",
        "              # Convert <h2> TITLE\\n name   <b>of</b>  \\tthe  (title) </h2> --> TITLE NAME OF THE \\(TITLE\\)\n",
        "              for h2 in h2s:\n",
        "                  h2_text = h2.get_text(separator = \"\")\n",
        "                  h2_text = re.sub(pattern, \" \", h2_text)\n",
        "                  h2_text = re.sub(r\"\\s+\", \" \", h2_text)\n",
        "                  h2_text = escape(h2_text)\n",
        "                  h2_text = h2_text.strip()\n",
        "                  h2_text = h2_text.upper()\n",
        "                  if h2_text == \"\":\n",
        "                    continue\n",
        "                  keys.append(h2_text)\n",
        "\n",
        "              if \"SUBSCRIBE\" in keys:\n",
        "                keys.remove(\"SUBSCRIBE\")\n",
        "              \n",
        "              if len(keys) == 0:\n",
        "                continue\n",
        "              \n",
        "              # Delete content after processing for saving memory\n",
        "              del document[\"content\"]\n",
        "              document[\"text\"] = re.sub(r\"\\s+\", \" \", document[\"text\"])\n",
        "\n",
        "\n",
        "              # Start of section creation part\n",
        "              sections = []\n",
        "              start_index = 0\n",
        "              for i in range(len(keys) - 1):\n",
        "                  # Search for occurrence of the next key, if not found search using regex\n",
        "                  end_index = document[\"text\"].find(keys[i + 1], start_index + 1)\n",
        "                  if end_index == -1:\n",
        "                    match = re.search(keys[i + 1], document[\"text\"][start_index:], re.IGNORECASE)\n",
        "                    interval = tuple(x + start_index for x in match.span())\n",
        "                    end_index = interval[0]\n",
        "\n",
        "                  section = create_section(keys[i], document[\"text\"], start_index, end_index)\n",
        "                  start_index = end_index\n",
        "                  sections.append(section)\n",
        "\n",
        "              end_index = len(document[\"text\"])\n",
        "              section = create_section(keys[-1], document[\"text\"], start_index, end_index)\n",
        "              sections.append(section)\n",
        "\n",
        "              document[\"sections\"] = sections\n",
        "\n",
        "              line_data.append(document)\n",
        "          \n",
        "          file_out.write(json.dumps(line_data) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGHQl4svRHJJ",
        "outputId": "39b7e452-fdb7-49a9-e056-df43ec35e2d6"
      },
      "outputs": [],
      "source": [
        "# Count documents and lines\n",
        "\n",
        "tot_docs = 0\n",
        "tot_lines = 0\n",
        "with open(fixed_file_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        line_documents = json.loads(line)\n",
        "        for document in line_documents:\n",
        "            tot_docs = tot_docs + 1\n",
        "        tot_lines = tot_lines + 1\n",
        "\n",
        "print(\"Tot docs: \" + str(tot_docs))\n",
        "print(\"Tot lines: \" + str(tot_lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvA69lEnbk4a",
        "outputId": "913ef5ae-c79b-4321-c921-c9404e84636c"
      },
      "outputs": [],
      "source": [
        "# Count and check how many sections needs to be truncated (> 512 words) and the number of sections, plus the length of longest section\n",
        "\n",
        "max_len = 512\n",
        "cont_section_truncated = 0\n",
        "len_longest_section = 0\n",
        "tot_sections = 0\n",
        "len_section_truncated = []\n",
        "with open(partial_out_path, \"r\") as file:\n",
        "  for line in file:\n",
        "    line_documents = json.loads(line)\n",
        "    for document in line_documents:\n",
        "      for section in document[\"sections\"]:\n",
        "        section_text = section[\"text\"]\n",
        "        tot_sections = tot_sections + 1\n",
        "        # Split the sentence in words and count them\n",
        "        n_words = len(section_text.split())\n",
        "        if n_words > max_len:\n",
        "          cont_section_truncated = cont_section_truncated + 1\n",
        "          len_section_truncated.append(n_words)\n",
        "          if n_words > len_longest_section:\n",
        "            len_longest_section = n_words\n",
        "\n",
        "print(\"cont_section_truncated: \", str(cont_section_truncated))\n",
        "print(\"len_longest_section: \", str(len_longest_section))\n",
        "print(\"tot_sections: \", str(tot_sections))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC-bIRzLoPNR"
      },
      "outputs": [],
      "source": [
        "def create_section_512(original_title, title_cont, char_start_index, char_end_index, text, offset):\n",
        "    section = {}\n",
        "    section[\"title\"] = original_title + \"-\" + str(title_cont)\n",
        "    section[\"start\"] = char_start_index + offset\n",
        "    section[\"end\"] = char_end_index + offset\n",
        "    section[\"text\"] = text[char_start_index: char_end_index]\n",
        "    return section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwi8Xh-4ahqA"
      },
      "outputs": [],
      "source": [
        "max_word_len = 512\n",
        "\n",
        "with open(final_out_path, \"w\") as file_out:\n",
        "    with open(partial_out_path, \"r\") as file:\n",
        "        for line in file:\n",
        "            line_documents = json.loads(line)\n",
        "            for document in line_documents:\n",
        "                doc_text = document[\"text\"]\n",
        "                sections = []\n",
        "                for section in document[\"sections\"]:\n",
        "\n",
        "                    section_text = section[\"text\"]\n",
        "                    \n",
        "                    # Divide text by words\n",
        "                    txt_words_split = section_text.split() \n",
        "                    n_words = len(txt_words_split)\n",
        "                    \n",
        "                    offset = section[\"start\"]\n",
        "                    char_start_index = 0\n",
        "                    word_start_index = 0\n",
        "                    \n",
        "                    # Divide sections in chunks of 512 words max\n",
        "                    if n_words > max_word_len:\n",
        "                        title_cont = 0\n",
        "                        n_sections = n_words // max_word_len + 1\n",
        "                        n_words_last_section = n_words % max_word_len\n",
        "\n",
        "                        split_last_section = 0\n",
        "\n",
        "                        # Avoid sections with less than 10 words by splitting the second-last in half\n",
        "                        if n_words_last_section < 10:\n",
        "                            split_last_section = 2\n",
        "\n",
        "                        while title_cont < n_sections - split_last_section:\n",
        "\n",
        "                            cur_words = txt_words_split[word_start_index: min(word_start_index + 512, len(txt_words_split))]\n",
        "\n",
        "                            char_end_index = sum([len(cur_word) + 1 for cur_word in cur_words]) + char_start_index\n",
        "\n",
        "                            new_section = create_section_512(section[\"title\"], title_cont, char_start_index, char_end_index, section_text, offset)\n",
        "                            sections.append(new_section)\n",
        "\n",
        "                            # Update the indexes\n",
        "                            word_start_index = word_start_index + 512\n",
        "                            char_start_index = char_end_index\n",
        "                            title_cont = title_cont + 1\n",
        "\n",
        "\n",
        "                        # Join the last two sections \n",
        "                        if split_last_section == 2:\n",
        "                            last_2sections_words = txt_words_split[word_start_index :]\n",
        "                            cut_point = len(last_2sections_words) // 2\n",
        "\n",
        "                            words_first_section = last_2sections_words[:cut_point]\n",
        "                            char_end_index = sum([len(cur_word) + 1 for cur_word in words_first_section]) + char_start_index\n",
        "                            new_section = create_section_512(section[\"title\"], title_cont, char_start_index, char_end_index, section_text, offset)\n",
        "\n",
        "                            title_cont = title_cont + 1\n",
        "                            char_start_index = char_end_index\n",
        "                            sections.append(new_section)\n",
        "\n",
        "                            words_second_section = last_2sections_words[cut_point:]\n",
        "                            char_end_index = sum([len(cur_word) + 1 for cur_word in words_second_section]) + char_start_index\n",
        "                            new_section = create_section_512(section[\"title\"], title_cont, char_start_index, char_end_index, section_text, offset)\n",
        "                            sections.append(new_section)\n",
        "                    else:\n",
        "                        sections.append(section)\n",
        "\n",
        "                document[\"sections\"] = sections\n",
        "\n",
        "            file_out.write(json.dumps(line_documents) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tot_sections = 0\n",
        "with open(final_out_path, \"r\") as file:\n",
        "  for line in file:\n",
        "    line_documents = json.loads(line)\n",
        "    for document in line_documents:\n",
        "      for section in document[\"sections\"]:\n",
        "        tot_sections = tot_sections + 1\n",
        "        \n",
        "print(\"tot_sections: \", str(tot_sections))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
